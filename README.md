# ğŸ”¬ ClassificaÃ§Ã£o QuÃ¢ntica Variacional com Data Re-uploading

> **Explorando o poder expressivo dos circuitos quÃ¢nticos atravÃ©s da tÃ©cnica de Data Re-uploading**

Este projeto investiga o impacto da tÃ©cnica de **Data Re-uploading** em classificadores quÃ¢nticos variacionais (VQC) aplicados a diferentes nÃ­veis de complexidade de dados: desde problemas linearmente separÃ¡veis atÃ© fronteiras de decisÃ£o altamente nÃ£o-lineares.

---

## ğŸ“‹ SumÃ¡rio

- [VisÃ£o Geral](#-visÃ£o-geral)
- [O que Ã© Data Re-uploading?](#-o-que-Ã©-data-re-uploading)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Datasets e Resultados](#-datasets-e-resultados)
- [Arquitetura dos Circuitos](#-arquitetura-dos-circuitos)
- [InstalaÃ§Ã£o e ExecuÃ§Ã£o](#-instalaÃ§Ã£o-e-execuÃ§Ã£o)
- [ConclusÃµes](#-conclusÃµes)
- [ReferÃªncias](#-referÃªncias)

---

## ğŸ¯ VisÃ£o Geral

O objetivo deste projeto Ã© demonstrar empiricamente como a tÃ©cnica de **Data Re-uploading** aumenta a expressividade de circuitos quÃ¢nticos variacionais, permitindo que eles aprendam fronteiras de decisÃ£o mais complexas.

### Por que isso importa?

Circuitos quÃ¢nticos com encoding tradicional (dados inseridos uma Ãºnica vez) tÃªm limitaÃ§Ãµes na representaÃ§Ã£o de funÃ§Ãµes nÃ£o-lineares. O Data Re-uploading supera essa limitaÃ§Ã£o ao re-encodar os dados clÃ¡ssicos em mÃºltiplas camadas do circuito, funcionando de forma anÃ¡loga Ã s camadas ocultas de uma rede neural clÃ¡ssica.

---

## ğŸ”„ O que Ã© Data Re-uploading?

### Conceito Fundamental

O **Data Re-uploading** Ã© uma tÃ©cnica proposta por PÃ©rez-Salinas et al. (2020) que permite que um circuito quÃ¢ntico atue como um aproximador universal de funÃ§Ãµes. A ideia central Ã© simples, mas poderosa:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENCODING TRADICIONAL                         â”‚
â”‚                                                                 â”‚
â”‚   |0âŸ© â”€â”€[Encoding(x)]â”€â”€[Layer 1]â”€â”€[Layer 2]â”€â”€...â”€â”€[MediÃ§Ã£o]    â”‚
â”‚                 â†‘                                               â”‚
â”‚           Dados entram                                          â”‚
â”‚           APENAS AQUI                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA RE-UPLOADING                            â”‚
â”‚                                                                 â”‚
â”‚   |0âŸ© â”€â”€[Enc(x)]â”€â”€[L1]â”€â”€[Enc(x)]â”€â”€[L2]â”€â”€[Enc(x)]â”€â”€...â”€â”€[Med]   â”‚
â”‚            â†‘              â†‘              â†‘                      â”‚
â”‚         Dados          Dados          Dados                     â”‚
â”‚      re-encodados   re-encodados   re-encodados                 â”‚
â”‚      em CADA camada!                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Por que funciona?

1. **Maior Expressividade**: Cada re-encoding cria uma nova "camada" de nÃ£o-linearidade
2. **Analogia com Redes Neurais**: Similar a ter mÃºltiplas camadas ocultas
3. **Aproximador Universal**: Com camadas suficientes, pode aproximar qualquer funÃ§Ã£o contÃ­nua

### ImplementaÃ§Ã£o no CÃ³digo

**Com Re-uploading** (dados dentro do loop):
```python
@qml.qnode(dev)
def circuito(weights, x):
    for i, layer_w in enumerate(weights):
        qml.AngleEmbedding(features=x, wires=range(n_qubits), rotation='Z')  # â† RE-ENCODING
        layer(layer_w)
    return qml.expval(qml.PauliZ(0))
```

**Sem Re-uploading** (dados fora do loop):
```python
@qml.qnode(dev)
def circuito(weights, x):
    qml.AngleEmbedding(features=x, wires=range(n_qubits), rotation='Z')  # â† ENCODING ÃšNICO
    for i, layer_w in enumerate(weights):
        layer(layer_w)
    return qml.expval(qml.PauliZ(0))
```

---

## ğŸ“ Estrutura do Projeto

```
Laciq_PS/
â”‚
â”œâ”€â”€ ğŸ“‚ Baseline - Blobs/
â”‚   â””â”€â”€ Blobs.ipynb              # Baseline: problema linearmente separÃ¡vel
â”‚
â”œâ”€â”€ ğŸ“‚ Moons/
â”‚   â”œâ”€â”€ moons_CRecupload.ipynb   # Moons COM Data Re-uploading
â”‚   â””â”€â”€ moonS_SReupload.ipynb    # Moons SEM Data Re-uploading
â”‚
â”œâ”€â”€ ğŸ“‚ Iris/
â”‚   â”œâ”€â”€ iris_CReupload.ipynb     # Iris COM Data Re-uploading
â”‚   â””â”€â”€ iris_SReupload.ipynb     # Iris SEM Data Re-uploading
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“Š Datasets e Resultados

### 1ï¸âƒ£ Blobs (Baseline)

| CaracterÃ­stica | Valor |
|----------------|-------|
| **Complexidade** | Linear |
| **Amostras** | 500 |
| **Classes** | 2 |
| **Qubits** | 1 |
| **Camadas** | 1 |
![Blobs](Result\blobs.png)
> ğŸ’¡ **Insight**: Por ser linearmente separÃ¡vel, nÃ£o hÃ¡ necessidade de circuitos complexos nem Data Re-uploading. Serve como baseline para validar a implementaÃ§Ã£o.

---

### 2ï¸âƒ£ Moons (NÃ£o-Linear)

| MÃ©trica | Sem Re-uploading | Com Re-uploading |
|---------|------------------|------------------|
| **Qubits** | 2 | 2 |
| **Camadas** | 6 | 6 |
| **ConvergÃªncia** | Mais lenta | Mais rÃ¡pida |
| **AcurÃ¡cia Final** | ~80-90% | **~100%** |
![Moons](Result\moons.png)

> ğŸ”¥ **Resultado chave**: O Data Re-uploading permite que o modelo alcance **100% de acurÃ¡cia** em problemas com fronteiras nÃ£o-lineares como o Moons!

---

### 3ï¸âƒ£ Iris (Multiclasse)

| MÃ©trica | Sem Re-uploading | Com Re-uploading |
|---------|------------------|------------------|
| **Qubits** | 4 | 4 |
| **Camadas** | 8 | 8 |
| **Classes** | 3 | 3 |
| **Learning Rate** | 0.04 | 0.0004 |
| **Framework** | PyTorch | PyTorch |

> ğŸ§  **EstratÃ©gia Multiclasse**: Utilizamos 3 "sub-classificadores" quÃ¢nticos, cada um com pesos especÃ­ficos para uma classe. A prediÃ§Ã£o final Ã© o argmax das 3 saÃ­das.

```python
# Estrutura de pesos: (n_classes, n_layers, n_qubits, 3)
shape_weights = (3, 8, 4, 3)  # 3 classificadores Ã— 8 camadas Ã— 4 qubits Ã— 3 parÃ¢metros
```

---

## ğŸ—ï¸ Arquitetura dos Circuitos

### Componentes Principais

1. **Encoding**: `AngleEmbedding` com rotaÃ§Ã£o Z
2. **Ansatz**: `StronglyEntanglingLayers` (rotaÃ§Ãµes + CNOTs)
3. **MediÃ§Ã£o**: Valor esperado de PauliZ

### VisualizaÃ§Ã£o do Circuito (Moons com Re-uploading)

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
q0: â”€â”¤ RZ(xâ‚€) â”œâ”€â”¤                    â”œâ”€â”¤ RZ(xâ‚€) â”œâ”€â”¤                    â”œâ”€ ... â”€â”¤ âŸ¨ZâŠ—ZâŸ©
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  StronglyEntanglingâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  StronglyEntanglingâ”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚       Layer        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚       Layer        â”‚
q1: â”€â”¤ RZ(xâ‚) â”œâ”€â”¤                    â”œâ”€â”¤ RZ(xâ‚) â”œâ”€â”¤                    â”œâ”€ ... â”€â”¤
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     
     â•°â”€â”€â”€â”€â”€â”€â”€â”€ Camada 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â•°â”€â”€â”€â”€â”€â”€â”€â”€ Camada 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

---

## ğŸ’» InstalaÃ§Ã£o e ExecuÃ§Ã£o

### Requisitos

```bash
pip install pennylane pennylane-numpy torch scikit-learn matplotlib seaborn tqdm
```

### ExecuÃ§Ã£o

1. Clone ou baixe o repositÃ³rio
2. Abra os notebooks no Jupyter ou VS Code
3. Execute as cÃ©lulas sequencialmente

### Ordem Recomendada

1. **Blobs.ipynb** - Entender o baseline
2. **moons_CRecupload.ipynb** - Ver o Re-uploading em aÃ§Ã£o
3. **moonS_SReupload.ipynb** - Comparar sem Re-uploading
4. **iris_CReupload.ipynb** - Problema multiclasse com Re-uploading
5. **iris_SReupload.ipynb** - Comparar sem Re-uploading

---

## ğŸ“ ConclusÃµes

### Principais Descobertas

1. **O Data Re-uploading Ã© essencial para problemas nÃ£o-lineares**
   - Sem ele, o modelo fica limitado a fronteiras de decisÃ£o simples
   - Com ele, conseguimos separar datasets como Moons com 100% de acurÃ¡cia

2. **Trade-off: Expressividade vs Complexidade**
   - Mais re-encodings = mais expressivo, mas mais custoso computacionalmente
   - Ã‰ preciso balancear o nÃºmero de camadas com o tempo de treinamento

3. **NormalizaÃ§Ã£o Ã© crÃ­tica**
   - Para `AngleEmbedding`, escalar os dados para `[0, Ï€]` melhorou significativamente a convergÃªncia
   - Dados fora dessa faixa limitam as rotaÃ§Ãµes do circuito

4. **PyTorch facilita problemas multiclasse**
   - A integraÃ§Ã£o PennyLane + PyTorch permite usar `CrossEntropyLoss` e otimizadores sofisticados

### ComparaÃ§Ã£o Visual

```
                    SEM RE-UPLOADING              COM RE-UPLOADING
                    
Expressividade:     â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ (40%)             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (100%)

Fronteiras:         Lineares/Simples             Altamente nÃ£o-lineares

Moons Accuracy:     ~80-90%                      ~100%

ConvergÃªncia:       InstÃ¡vel                     EstÃ¡vel e rÃ¡pida
```

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido como parte do processo seletivo LACIQ.

---